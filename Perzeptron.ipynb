{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from ast import literal_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 100\n",
    "num_workers = 0\n",
    "N = 1370\n",
    "annotation_train = 'data/data_se/data_train.txt'\n",
    "annotation_val = 'data/data_se/data_test.txt'\n",
    "epoch = 100\n",
    "Loss = nn.CrossEntropyLoss()\n",
    "lr = 0.0001\n",
    "best_Loss_regression = 10000\n",
    "best_model_wts = None\n",
    "best_model_ind = 0\n",
    "best_model_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, annotation, N):\n",
    "        self.landmarks_frame = pd.read_csv(annotation)\n",
    "        self.N = N\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        i_data = literal_eval(self.landmarks_frame.iloc[idx, 1])\n",
    "        i = random.choice(i_data)\n",
    "        data = np.zeros(self.N,dtype = np.float32)\n",
    "        for d in i_data:\n",
    "            data[d] += 1\n",
    "        data[i] -= 1\n",
    "        return data, i\n",
    "train_dataset = MyDataset(annotation_train, N)\n",
    "val_dataset = MyDataset(annotation_val, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, drop_last=True, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronNet(nn.Module):\n",
    "    def __init__(self, N):\n",
    "        super(PerceptronNet, self).__init__()\n",
    "        self.L1 = nn.Linear(N, 2*N)\n",
    "        self.L2 = nn.Linear(2*N, 2*N)\n",
    "        self.L3 = nn.Linear(2*N, N)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.Softmax = nn.Softmax(1)\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.L1(x))\n",
    "        x = self.relu(self.L2(x))\n",
    "        x = self.relu(self.L3(x))\n",
    "        return x\n",
    "net = PerceptronNet(N).to(device)\n",
    "optimizer = optim.AdamW(net.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.5,patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(epoch):\n",
    "    for dataloader, phase in zip([train_dataloader, val_dataloader], [\"T\", \"V\"]):\n",
    "        eposh_loss = 0\n",
    "        if phase == \"T\": net.train()\n",
    "        if phase == \"V\": net.eval()\n",
    "        for data, laibl in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(phase == \"T\"):\n",
    "                out = net(data.to(device))\n",
    "                # print(out.shape, laibl.shape)\n",
    "                loss = Loss(out, laibl.to(device))\n",
    "                # print(out, laibl, loss)\n",
    "                if phase == 'T':\n",
    "                    # Вычислить градиенты\n",
    "                    loss.backward()\n",
    "                    # Обновить веса\n",
    "                    optimizer.step()\n",
    "                eposh_loss+=loss.item()\n",
    "        if phase == \"T\": \n",
    "            print(f\"{iter} {phase} {eposh_loss / len(dataloader):0.3f}\", end=\" \")\n",
    "        else:\n",
    "            scheduler.step(eposh_loss)\n",
    "            print(f\"{phase} {eposh_loss / len(dataloader):0.3f}\")\n",
    "            if eposh_loss < best_Loss_regression:\n",
    "                best_model_wts = net.state_dict()\n",
    "                best_model_ind = iter\n",
    "                best_Loss_regression = eposh_loss\n",
    "net.load_state_dict(best_model_wts)\n",
    "torch.save(best_model_wts, 'model.tar')\n",
    "print(best_model_ind, best_Loss_regression / len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 6.810856623152282\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(best_model_wts)\n",
    "torch.save(best_model_wts, 'model.tar')\n",
    "print(best_model_ind, best_Loss_regression / len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc top 1 tensor(0.0992, device='cuda:0')\n",
      "Acc top 3 0.10831234256926953\n",
      "Acc top 5 0.11122635452165754\n",
      "Acc top 10 0.11537511730132859\n",
      "Acc top 50 0.16666666666666666\n",
      "Acc top 500 0.5693436064602163\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "trye = 0\n",
    "trye_3 = 0\n",
    "trye_5 = 0\n",
    "trye_10 = 0\n",
    "trye_50 = 0\n",
    "trye_500 = 0\n",
    "data_test = pd.read_csv(annotation_val)\n",
    "for d in data_test['name_id'].values:\n",
    "    mas = literal_eval(d)\n",
    "    for i, m in enumerate(mas):\n",
    "        buf = np.zeros((N),dtype = np.float32)\n",
    "        for el in [*mas[:i],*mas[i+1:]]:\n",
    "            buf[el] +=1\n",
    "        buf[m] -=1\n",
    "        inpyt = torch.tensor(buf).to(device)\n",
    "        out = net(inpyt)\n",
    "        trye += mas[i] == out.argmax()\n",
    "        trye_3 += mas[i] in out.argsort(descending=True)[:3]\n",
    "        trye_5 += mas[i] in out.argsort(descending=True)[:5]\n",
    "        trye_10 += mas[i] in out.argsort(descending=True)[:10]\n",
    "        trye_50 += mas[i] in out.argsort(descending=True)[:50]\n",
    "        trye_500 += mas[i] in out.argsort(descending=True)[:500]\n",
    "        n+=1\n",
    "print('Acc top 1', trye / n)\n",
    "print('Acc top 3',trye_3 / n)\n",
    "print('Acc top 5',trye_5 / n)\n",
    "print('Acc top 10',trye_10 / n)\n",
    "print('Acc top 50',trye_50 / n)\n",
    "print('Acc top 500',trye_500 / n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
